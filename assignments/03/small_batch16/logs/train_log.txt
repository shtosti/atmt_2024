INFO: COMMAND: train.py --train-on-tiny --data data/en-fr/prepared --source-lang fr --target-lang en --save-dir assignments/03/small_hyperparameter-tuning_batch/checkpoints --log-file assignments/03/small_hyperparameter-tuning_batch/logs/train_log.txt --batch-size 16
INFO: Arguments: {'cuda': False, 'data': 'data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 16, 'train_on_tiny': True, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': 'assignments/03/small_hyperparameter-tuning_batch/logs/train_log.txt', 'save_dir': 'assignments/03/small_hyperparameter-tuning_batch/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (fr) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 7.488 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.884 | clip 0.6032
INFO: Epoch 000: valid_loss 5.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 342
INFO: Epoch 001: loss 5.282 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 11 | clip 1
INFO: Epoch 001: valid_loss 5.32 | num_tokens 9.14 | batch_size 500 | valid_perplexity 205
INFO: Epoch 002: loss 5.017 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.72 | clip 1
INFO: Epoch 002: valid_loss 5.28 | num_tokens 9.14 | batch_size 500 | valid_perplexity 196
INFO: Epoch 003: loss 4.864 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.77 | clip 1
INFO: Epoch 003: valid_loss 5.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 183
INFO: Epoch 004: loss 4.758 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.82 | clip 1
INFO: Epoch 004: valid_loss 5.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 159
INFO: Epoch 005: loss 4.661 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.54 | clip 1
INFO: Epoch 005: valid_loss 5.01 | num_tokens 9.14 | batch_size 500 | valid_perplexity 150
INFO: Epoch 006: loss 4.605 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.69 | clip 1
INFO: Epoch 006: valid_loss 4.96 | num_tokens 9.14 | batch_size 500 | valid_perplexity 143
INFO: Epoch 007: loss 4.551 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.65 | clip 1
INFO: Epoch 007: valid_loss 4.91 | num_tokens 9.14 | batch_size 500 | valid_perplexity 136
INFO: Epoch 008: loss 4.497 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.45 | clip 1
INFO: Epoch 008: valid_loss 4.89 | num_tokens 9.14 | batch_size 500 | valid_perplexity 133
INFO: Epoch 009: loss 4.445 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.29 | clip 1
INFO: Epoch 009: valid_loss 4.88 | num_tokens 9.14 | batch_size 500 | valid_perplexity 131
INFO: Epoch 010: loss 4.41 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.25 | clip 1
INFO: Epoch 010: valid_loss 4.84 | num_tokens 9.14 | batch_size 500 | valid_perplexity 126
INFO: Epoch 011: loss 4.357 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.17 | clip 1
INFO: Epoch 011: valid_loss 4.81 | num_tokens 9.14 | batch_size 500 | valid_perplexity 122
INFO: Epoch 012: loss 4.314 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.02 | clip 1
INFO: Epoch 012: valid_loss 4.8 | num_tokens 9.14 | batch_size 500 | valid_perplexity 121
INFO: Epoch 013: loss 4.269 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.03 | clip 1
INFO: Epoch 013: valid_loss 4.78 | num_tokens 9.14 | batch_size 500 | valid_perplexity 119
INFO: Epoch 014: loss 4.234 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.13 | clip 1
INFO: Epoch 014: valid_loss 4.75 | num_tokens 9.14 | batch_size 500 | valid_perplexity 116
INFO: Epoch 015: loss 4.196 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.06 | clip 1
INFO: Epoch 015: valid_loss 4.72 | num_tokens 9.14 | batch_size 500 | valid_perplexity 112
INFO: Epoch 016: loss 4.156 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.966 | clip 1
INFO: Epoch 016: valid_loss 4.71 | num_tokens 9.14 | batch_size 500 | valid_perplexity 111
INFO: Epoch 017: loss 4.108 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.27 | clip 1
INFO: Epoch 017: valid_loss 4.68 | num_tokens 9.14 | batch_size 500 | valid_perplexity 108
INFO: Epoch 018: loss 4.073 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.07 | clip 1
INFO: Epoch 018: valid_loss 4.65 | num_tokens 9.14 | batch_size 500 | valid_perplexity 104
INFO: Epoch 019: loss 4.029 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.17 | clip 1
INFO: Epoch 019: valid_loss 4.62 | num_tokens 9.14 | batch_size 500 | valid_perplexity 101
INFO: Epoch 020: loss 3.995 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.823 | clip 1
INFO: Epoch 020: valid_loss 4.61 | num_tokens 9.14 | batch_size 500 | valid_perplexity 100
INFO: Epoch 021: loss 3.954 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.848 | clip 1
INFO: Epoch 021: valid_loss 4.59 | num_tokens 9.14 | batch_size 500 | valid_perplexity 98.7
INFO: Epoch 022: loss 3.919 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 11.62 | clip 1
INFO: Epoch 022: valid_loss 4.58 | num_tokens 9.14 | batch_size 500 | valid_perplexity 97.5
INFO: Epoch 023: loss 3.878 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10 | clip 1
INFO: Epoch 023: valid_loss 4.56 | num_tokens 9.14 | batch_size 500 | valid_perplexity 95.7
INFO: Epoch 024: loss 3.861 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.18 | clip 1
INFO: Epoch 024: valid_loss 4.53 | num_tokens 9.14 | batch_size 500 | valid_perplexity 92.9
INFO: Epoch 025: loss 3.814 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.865 | clip 1
INFO: Epoch 025: valid_loss 4.54 | num_tokens 9.14 | batch_size 500 | valid_perplexity 93.6
INFO: Epoch 026: loss 3.785 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.974 | clip 1
INFO: Epoch 026: valid_loss 4.51 | num_tokens 9.14 | batch_size 500 | valid_perplexity 91.2
INFO: Epoch 027: loss 3.751 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.18 | clip 1
INFO: Epoch 027: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88.1
INFO: Epoch 028: loss 3.715 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.01 | clip 1
INFO: Epoch 028: valid_loss 4.47 | num_tokens 9.14 | batch_size 500 | valid_perplexity 87.5
INFO: Epoch 029: loss 3.677 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 9.961 | clip 1
INFO: Epoch 029: valid_loss 4.48 | num_tokens 9.14 | batch_size 500 | valid_perplexity 88
INFO: Epoch 030: loss 3.654 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.09 | clip 1
INFO: Epoch 030: valid_loss 4.46 | num_tokens 9.14 | batch_size 500 | valid_perplexity 86.3
INFO: Epoch 031: loss 3.621 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.4 | clip 1
INFO: Epoch 031: valid_loss 4.44 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84.8
INFO: Epoch 032: loss 3.59 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.21 | clip 1
INFO: Epoch 032: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.4
INFO: Epoch 033: loss 3.559 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 31 | clip 1
INFO: Epoch 033: valid_loss 4.45 | num_tokens 9.14 | batch_size 500 | valid_perplexity 85.5
INFO: Epoch 034: loss 3.538 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.27 | clip 1
INFO: Epoch 034: valid_loss 4.42 | num_tokens 9.14 | batch_size 500 | valid_perplexity 83.5
INFO: Epoch 035: loss 3.507 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.29 | clip 1
INFO: Epoch 035: valid_loss 4.43 | num_tokens 9.14 | batch_size 500 | valid_perplexity 84
INFO: Epoch 036: loss 3.482 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.32 | clip 1
INFO: Epoch 036: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.6
INFO: Epoch 037: loss 3.45 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.35 | clip 1
INFO: Epoch 037: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.9
INFO: Epoch 038: loss 3.428 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.21 | clip 1
INFO: Epoch 038: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 82.2
INFO: Epoch 039: loss 3.397 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.53 | clip 1
INFO: Epoch 039: valid_loss 4.41 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.9
INFO: Epoch 040: loss 3.383 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.78 | clip 1
INFO: Epoch 040: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.5
INFO: Epoch 041: loss 3.359 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.62 | clip 1
INFO: Epoch 041: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.6
INFO: Epoch 042: loss 3.322 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.25 | clip 1
INFO: Epoch 042: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80.5
INFO: Epoch 043: loss 3.299 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.37 | clip 1
INFO: Epoch 043: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81
INFO: Epoch 044: loss 3.281 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.23 | clip 1
INFO: Epoch 044: valid_loss 4.39 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81
INFO: Epoch 045: loss 3.264 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.7 | clip 1
INFO: Epoch 045: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 80
INFO: Epoch 046: loss 3.231 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.49 | clip 1
INFO: Epoch 046: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.8
INFO: Epoch 047: loss 3.205 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.73 | clip 1
INFO: Epoch 047: valid_loss 4.4 | num_tokens 9.14 | batch_size 500 | valid_perplexity 81.5
INFO: Epoch 048: loss 3.19 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.69 | clip 1
INFO: Epoch 048: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.8
INFO: Epoch 049: loss 3.165 | lr 0.0003 | num_tokens 9.368 | batch_size 15.87 | grad_norm 10.77 | clip 1
INFO: Epoch 049: valid_loss 4.38 | num_tokens 9.14 | batch_size 500 | valid_perplexity 79.8
INFO: No validation set improvements observed for 3 epochs. Early stop!
